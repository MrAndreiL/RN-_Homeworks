{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W_6JvRc0H3RH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "\n",
        "def download_mnist(is_train: bool):\n",
        "    dataset = MNIST(root='./data',\n",
        "                    transform=lambda x: np.array(x).flatten(),\n",
        "                    download=True,\n",
        "                    train=is_train)\n",
        "\n",
        "    mnist_data = []\n",
        "    mnist_labels = []\n",
        "    for image, labels in dataset:\n",
        "        mnist_data.append(image)\n",
        "        mnist_labels.append(labels)\n",
        "    return np.array(mnist_data), np.array(mnist_labels)\n",
        "\n",
        "\n",
        "X_train, y_train = download_mnist(True)\n",
        "X_test, y_test = download_mnist(False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeData(data: np.array) -> np.array:\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "\n",
        "X_train_norm = normalizeData(X_train)\n",
        "X_test_norm = normalizeData(X_test)"
      ],
      "metadata": {
        "id": "w6fwl0NKIFPl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createEntryArray(value: int) -> np.array:\n",
        "    entry = [0] * 10\n",
        "    entry[value] = 1\n",
        "    return entry\n",
        "\n",
        "\n",
        "def oneHotEncoding(data: np.array) -> np.array:\n",
        "    # Create new numpy array.\n",
        "    oneHotData = []\n",
        "    # Iterate through each element, and build one hot version.\n",
        "    for entry in data:\n",
        "        oneHotData.append(createEntryArray(entry))\n",
        "    return np.array(oneHotData)\n",
        "\n",
        "\n",
        "y_train_oneHot = oneHotEncoding(y_train)\n",
        "y_test_oneHot = oneHotEncoding(y_test)"
      ],
      "metadata": {
        "id": "b0d_oxx1IOWW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitBatches(df: np.array, batch_size: int) -> np.array:\n",
        "    splitStart = 0\n",
        "    splitEnd = batch_size\n",
        "    batches = df.shape[0] // batch_size\n",
        "    batchdf = []\n",
        "    for i in range(batches):\n",
        "        batchdf.append(list(df[splitStart : splitEnd]))\n",
        "        splitStart = splitEnd\n",
        "        splitEnd += batch_size\n",
        "    if (splitEnd < df.shape[0]):\n",
        "        batchdf.append(list(df[splitEnd : df.shape[0]]))\n",
        "    return np.array(batchdf, dtype=np.float128)\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "X_train_split = splitBatches(X_train_norm, batch_size)\n",
        "y_train_split = splitBatches(y_train_oneHot, batch_size)\n",
        "X_test_split = splitBatches(X_test_norm, batch_size)\n",
        "y_test_split = splitBatches(y_test_oneHot, batch_size)"
      ],
      "metadata": {
        "id": "qlKk897MIR6Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forwardPropagation(X_train: np.array,\n",
        "                       weights: np.array,\n",
        "                       biases: np.array) -> np.array:\n",
        "    z = weights @ X_train.T\n",
        "    linEq = z.T + biases\n",
        "    return softmax(linEq)\n",
        "\n",
        "\n",
        "def softmax(scores: np.array) -> np.array:\n",
        "    s = np.max(scores, axis=1)\n",
        "    s = s[:, np.newaxis]\n",
        "    e_x = np.exp(scores - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis]\n",
        "    return e_x / div\n",
        "\n",
        "\n",
        "def gradientDescentUpdate(X_train: np.array,\n",
        "                          weights: np.array,\n",
        "                          biases: np.array,\n",
        "                          l_rate: float,\n",
        "                          error: np.array) -> tuple[np.array, np.array]:\n",
        "    biases = biases + l_rate * error.sum(axis=0)\n",
        "    weights = weights + l_rate * error.T @ X_train\n",
        "    return (weights, biases)"
      ],
      "metadata": {
        "id": "ldiLQZ1aIV8A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainNeuralNetwork(X_train: np.array,\n",
        "                       y_train: np.array,\n",
        "                       batch_size: int,\n",
        "                       l_rate: float,\n",
        "                       epochs: int) -> tuple[np.array, np.array]:\n",
        "    # Initialize weights and biases.\n",
        "    weights = np.zeros((10, 784), dtype=np.float128)\n",
        "    biases  = np.zeros((10), dtype=np.float128)\n",
        "    # Iterate through epochs.\n",
        "    print(\"Start training\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch={epoch}\")\n",
        "        # For each batch, perform forward propagation, gradient descent and update.\n",
        "        for batch in range(batch_size):\n",
        "            # Prepare data for forward propagation.\n",
        "            xBatch = X_train[batch]\n",
        "            yBatch = y_train[batch]\n",
        "\n",
        "            # Make prediction using forward propagation.\n",
        "            prediction = forwardPropagation(xBatch, weights, biases)\n",
        "\n",
        "            # Compute error.\n",
        "            error = yBatch - prediction\n",
        "\n",
        "            # Update weights and biases using gradient descent.\n",
        "            (weightsBatch, biasesBatch) = gradientDescentUpdate(xBatch, weights, biases, l_rate, error)\n",
        "            weights = weightsBatch\n",
        "            biases = biasesBatch\n",
        "    print(\"Training ended.\")\n",
        "    return (weights, biases)"
      ],
      "metadata": {
        "id": "w705skxwIcdC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_rate = 0.001\n",
        "epochs = 60\n",
        "(weights, biases) = trainNeuralNetwork(X_train_split, y_train_split, batch_size, l_rate, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VkrAptvIhiP",
        "outputId": "44996f98-0cdd-4854-cc34-c60f272fcec3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training\n",
            "Epoch=0\n",
            "Epoch=1\n",
            "Epoch=2\n",
            "Epoch=3\n",
            "Epoch=4\n",
            "Epoch=5\n",
            "Epoch=6\n",
            "Epoch=7\n",
            "Epoch=8\n",
            "Epoch=9\n",
            "Epoch=10\n",
            "Epoch=11\n",
            "Epoch=12\n",
            "Epoch=13\n",
            "Epoch=14\n",
            "Epoch=15\n",
            "Epoch=16\n",
            "Epoch=17\n",
            "Epoch=18\n",
            "Epoch=19\n",
            "Epoch=20\n",
            "Epoch=21\n",
            "Epoch=22\n",
            "Epoch=23\n",
            "Epoch=24\n",
            "Epoch=25\n",
            "Epoch=26\n",
            "Epoch=27\n",
            "Epoch=28\n",
            "Epoch=29\n",
            "Epoch=30\n",
            "Epoch=31\n",
            "Epoch=32\n",
            "Epoch=33\n",
            "Epoch=34\n",
            "Epoch=35\n",
            "Epoch=36\n",
            "Epoch=37\n",
            "Epoch=38\n",
            "Epoch=39\n",
            "Epoch=40\n",
            "Epoch=41\n",
            "Epoch=42\n",
            "Epoch=43\n",
            "Epoch=44\n",
            "Epoch=45\n",
            "Epoch=46\n",
            "Epoch=47\n",
            "Epoch=48\n",
            "Epoch=49\n",
            "Epoch=50\n",
            "Epoch=51\n",
            "Epoch=52\n",
            "Epoch=53\n",
            "Epoch=54\n",
            "Epoch=55\n",
            "Epoch=56\n",
            "Epoch=57\n",
            "Epoch=58\n",
            "Epoch=59\n",
            "Training ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batches = 100\n",
        "\n",
        "def getAccuracy(prediction: np.array, target: np.array) -> float:\n",
        "    length = prediction.shape[0]\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(length):\n",
        "        if (np.argmax(prediction[i]) == np.argmax(target[i])):\n",
        "            correct += 1\n",
        "    return correct / length\n",
        "\n",
        "meanAccuracy = 0.0\n",
        "for batch in range(batches):\n",
        "    xBatch = X_test_split[batch]\n",
        "    yBatch = y_test_split[batch]\n",
        "\n",
        "    predictionProb = forwardPropagation(xBatch, weights, biases)\n",
        "\n",
        "    predictionAcc = getAccuracy(predictionProb, yBatch)\n",
        "    meanAccuracy += predictionAcc\n",
        "print(f\"Accuracy={meanAccuracy / batches}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUPWvor2LL4F",
        "outputId": "333dca82-a4cf-431f-d9ff-eca4b31f05e1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy=0.9107\n"
          ]
        }
      ]
    }
  ]
}